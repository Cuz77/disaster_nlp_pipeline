{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a97fa089",
   "metadata": {},
   "source": [
    "## ML Pipeline\n",
    "\n",
    "This file has been instrumental in making decisions and developing the ML pipeline stored in the python file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ed232",
   "metadata": {},
   "source": [
    "### Import libraries and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0475fa0d",
   "metadata": {},
   "source": [
    "Let's start with downloading and importing libraries and then set up some static values and warning options and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e299c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JakubBelow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JakubBelow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JakubBelow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords') \n",
    "\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize, punkt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Prevent sklearn from printing ConvergenceWarning (due to max iterations limit)\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category = ConvergenceWarning)\n",
    "\n",
    "# a static value to detect hyperlinks\n",
    "URL_REGEX = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a29e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "engine = create_engine('sqlite:///DB/disaster_messages.db')\n",
    "df = pd.read_sql_table('DB/disaster_messages', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1661022a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        0      0            0             0                 0  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66afa4e8",
   "metadata": {},
   "source": [
    "### Process data and engineer features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef48cf44",
   "metadata": {},
   "source": [
    "We don't have much feature engineering to do here, so let's define a tokenizer to be used in the pipeline that will:\n",
    "- replace all hyperlinks with a placeholder\n",
    "- tokenize words\n",
    "- lemmatize and transform tokens to lowercase\n",
    "- clean tokens from any stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9494d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 'child_alone' feature that doesn't seem to appear at all in the dataset\n",
    "features = df.loc[:,'related':].columns.to_list()\n",
    "features.remove('child_alone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200ff9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Desc: Returns cleaned and lemmatized tokens from a text to be used by an NLP vectorizer\n",
    "    \n",
    "        Parameters:\n",
    "            text (str): a document to be processed (e.g. a twitter message)\n",
    "        Returns:\n",
    "            clean_tokens (list[str]): a list of cleaned and lemmatized word tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # find and replace all hyperlinks\n",
    "    urls = re.findall(URL_REGEX, text)\n",
    "    \n",
    "    for url in urls:\n",
    "        text = text.replace(url, '<url>')\n",
    "        \n",
    "    # tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize and clean words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = [lemmatizer.lemmatize(tok).lower().strip() for tok in tokens]\n",
    "    clean_tokens = [tok for tok in clean_tokens if tok not in STOP_WORDS]\n",
    "        \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7369df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X = df['message']\n",
    "y = df[features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c9b1947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55053ae3",
   "metadata": {},
   "source": [
    "### Build the pipeline and quickly assess three different models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72527128",
   "metadata": {},
   "source": [
    "It's worth checking different estimators for multitarget mnultioutput learning model to choose one that's performing hte best before tuning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a73ee11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to build a pipeline with given estimator\n",
    "def model_pipeline(model):\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize, token_pattern=None)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(estimator=model))\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a18e0bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=1000):\n",
      "\n",
      "success:\n",
      " precision    0.939473\n",
      "recall       0.947385\n",
      "f1-score     0.936945\n",
      "dtype: float64 \n",
      "\n",
      "\n",
      "MultinomialNB():\n",
      "\n",
      "success:\n",
      " precision    0.906830\n",
      "recall       0.932930\n",
      "f1-score     0.908542\n",
      "dtype: float64 \n",
      "\n",
      "\n",
      "SGDClassifier():\n",
      "\n",
      "success:\n",
      " precision    0.939719\n",
      "recall       0.948921\n",
      "f1-score     0.938432\n",
      "dtype: float64 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run three different estimators to check which one's the best\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    MultinomialNB(),\n",
    "    SGDClassifier()\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    pipeline = model_pipeline(model)\n",
    "    print(f'{model}:\\n')\n",
    "    # train classifier\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    pred_df = pd.DataFrame(y_pred, columns=y_test.columns)\n",
    "    report_df = pd.DataFrame(columns=['precision', 'recall', 'f1-score'])\n",
    "\n",
    "    for col in pred_df.columns:\n",
    "        scores = classification_report(y_test[col], pred_df[col], output_dict=True, zero_division=0)['weighted avg']\n",
    "        precision, recall, f1_score, _ = [score for score in scores.values()]\n",
    "        report_df.loc[len(report_df)] = [precision, recall, f1_score]\n",
    "\n",
    "    report_df.index = pred_df.columns\n",
    "    report_df\n",
    "    \n",
    "    print('success:\\n', report_df.mean(), '\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1bc3ba",
   "metadata": {},
   "source": [
    "SGDClassifier seems to pereform the best. Since we only have binary features, the feature scaling is not a problem here (it's sensistive to it).\n",
    "\n",
    "Now, let's use this estimator to check fot best hyperparameters. We will try with the following three:\n",
    "1. penalty\n",
    "2. loss\n",
    "3. max iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = model_pipeline(SGDClassifier())\n",
    "parameters = {'clf__estimator__penalty' : ['l1', 'l2', 'elasticnet'],\n",
    "              'clf__estimator__loss': ['hinge', 'log_loss', 'squared_hinge', 'perceptron'],\n",
    "              'clf__estimator__max_iter' : [200, 500, 1000]\n",
    "             }\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "best_clf = cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19eaad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23a0491",
   "metadata": {},
   "source": [
    "It took us over 20 minutes, but we have the winners. The loss function will be 'hinge' with 500 max iterations and l2 penalty regulazer. Finally, let's estimate precision, recall, and f1-score for each target class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3091f9",
   "metadata": {},
   "source": [
    "### Report scores for each target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5393ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict y values and the test sample\n",
    "y_pred = best_clf.predict(X_test)\n",
    "pred_df = pd.DataFrame(y_pred, columns=y_test.columns)\n",
    "report_df = pd.DataFrame(columns=['precision', 'recall', 'f1-score'])\n",
    "\n",
    "for col in pred_df.columns:\n",
    "    scores = classification_report(y_test[col], pred_df[col], output_dict=True, zero_division=0)['weighted avg']\n",
    "    precision, recall, f1_score, _ = [score for score in scores.values()]\n",
    "    report_df.loc[len(report_df)] = [precision, recall, f1_score]\n",
    "\n",
    "report_df.index = pred_df.columns\n",
    "report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf7052",
   "metadata": {},
   "source": [
    "8 out of 35 classes are never predicted for the training data set. This is a potential pain point for the next iteration of the model. Oversampling might possibly help with the issue. Otherwise, a business decision may be made to forfeit these features for the time being altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f84b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get % of predicted classes\n",
    "pred_counts = pd.DataFrame(y_pred, columns=y_test.columns)\n",
    "pred_counts = pred_counts.sum().sort_values() / pred_counts.shape[0]\n",
    "\n",
    "# get % of actual classes\n",
    "test_counts = pd.DataFrame(y_test, columns=y_test.columns)\n",
    "test_counts = test_counts.sum().sort_values() / test_counts.shape[0]\n",
    "\n",
    "#create a new dataframe to compare\n",
    "classes_prevalencs_df = pd.concat([test_counts, pred_counts], axis=1)\n",
    "classes_prevalencs_df.columns = ['actual prevalence', 'predicted_pct']\n",
    "classes_prevalencs_df.sort_values(by='predicted_pct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4318f606",
   "metadata": {},
   "source": [
    "Interestingly, some of the classes that were not predicted at all are not actually that underrepresented. For instance, the \"other_infrastructure\" class does account for approx. 4.5% of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a48713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the model\n",
    "import pickle\n",
    "pickle.dump(best_clf, open(f'models/model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8504bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3cbd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2cdd46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3b7e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
